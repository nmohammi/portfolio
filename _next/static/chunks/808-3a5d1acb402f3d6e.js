"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[808],{2675:function(e,n,a){a.d(n,{$:function(){return i}});var t=a(7437);a(2265);let i=e=>{let{children:n,id:a,className:i="",title:o,subtitle:s}=e;return(0,t.jsx)("section",{id:a,className:"py-16 ".concat(i),children:(0,t.jsxs)("div",{className:"container",children:[(o||s)&&(0,t.jsxs)("div",{className:"text-center mb-12",children:[o&&(0,t.jsx)("h2",{className:"text-3xl md:text-4xl font-bold text-gray-900 mb-4",children:o}),s&&(0,t.jsx)("p",{className:"text-lg text-gray-600 max-w-2xl mx-auto",children:s})]}),n]})})}},1045:function(e){e.exports=JSON.parse('[{"id":1,"position":"GenAI <span className=\\"font-serif\\">&</span> Data Analytics Engineer","company":"Opteamis - French Tech 120 scale","period":"Mars 2025 - Present","description":"• Built dbt data pipelines (300+ tables, KPIs, semantic layer)\\n• Developed AI solutions: Talent Search, Matchmaker, RatePricer, CV Parser, Vector DB infra\\n• Deployed MCP server → ChatGPT-like interface for any request on our platform (internal API)\\n• Created Agent Sourcer → automated sourcing + scoring (LinkedIn, others)\\n• Delivered AI/VMS tools to Manpower, Axa, EDF and other clients across 120+ countries","detailedDescription":"As a GenAI <span className=\\"font-serif\\">&</span> Data Analytics Engineer at Opteamis, I spearheaded the development of cutting-edge AI solutions and data infrastructure that powers talent acquisition across 120+ countries.\\n\\n**Data Pipeline <span className=\\"font-serif\\">&</span> Infrastructure:**\\n\\n• **Data Pipeline Configuration** for automated data ingestion via Airbyte from Opteamis and MyTopManager platforms, enabling the feeding of over 300 tables in a Data Lake based on BigQuery and Google Cloud Storage\\n\\n• **dbt Pipeline Implementation** based on a layered architecture (with staging, intermediate, and datamart models) to structure and ensure reliable data transformations and cleaning from the datalake to the datawarehouse\\n\\n• **Robust Documentation and Data Quality Tests** via dbt to guarantee transformation reliability, then production deployment to feed dashboards on Google Sheets and Looker\\n\\n• **Business KPIs Implementation** via dbt stack (Business Metrics) and reliable financial KPIs (revenue, margin, daily rate, closing rate) and dashboard construction for performance monitoring and productivity ratio calculation\\n\\n• **Dedicated Datamarts Creation** for internal stakeholders (Sales, Office, Marketing teams) as well as strategic clients like AXA, Manpower, Alten, Extia, etc.\\n\\n• **Workshop Facilitation and Collaboration** with CEO, CTO, market director, commercial teams to identify, align and document a common repository of over 50 key KPIs, with shared and unified definitions for everyone\\n\\n• **Billing Tables Redesign** in the data model to correct historical inconsistencies, and launch forecasting analyses (M+1, M+3, 12 months) based on average daily rates, working days and staffing assumptions\\n\\n**NLP <span className=\\"font-serif\\">&</span> Data Processing:**\\n\\n• **NLP Processing of over 300,000 Business Tags** to unify, clean (errors, duplicates, variations) and group titles, with embedding via Hugging Face models, HDBSCAN/KMeans clustering, then LLM prompting (Gemini)\\n\\n• **Cluster Mapping** to a structured repository of 13 business domains, 25 specialties and 11 activity sectors\\n\\n• **Scoping Session Facilitation** with business teams to define grouping logic, validate mappings and consolidate a robust and reusable repository\\n\\n**AI Product Development:**\\n\\n• **First AI Building Block Creation** for the company through the implementation of a vector database dedicated to storing semantic embeddings of over 200,000 CVs\\n\\n• **Complete Benchmark** (Pinecone, Qdrant, Weaviate) to select the most optimal solution in terms of cost, performance and scalability\\n\\n• **Third-party SaaS Solution Replacement** at €12,000/month by a stable internal infrastructure at less than €1,000/year, with superior performance\\n\\n• **RatePricer Development**: a daily rate prediction algorithm from CVs and Jobs, with 80% accuracy, thanks to state-of-the-art embeddings (Qwen 3)\\n\\n• **Light Supervised Model Training** (CatBoost, LightGBM) on vectors, with targeted data augmentation techniques and dimension reduction by PCA\\n\\n• **Next-generation Talent Search Solution Design**: Development of an intelligent search engine based on Qwen 3 embeddings, integrating query vectorization and cosine similarity search\\n\\n• **Intelligent CV Parser Development** based on LLMs: automatic extraction of key entities (experiences, education, languages, tools, certifications, etc.) and generation of an enriched and structured schema\\n\\n• **Intelligent Matching Solution Design** between profiles and job offers (CV ↔ Jobs), used by VMS clients as well as internal Sales teams\\n\\n• **Processing of over 200,000 CVs and 10,000 job offers** via this system, with a clear improvement in recommendation relevance\\n\\n**API <span className=\\"font-serif\\">&</span> Integration:**\\n\\n• **Complete AI API Creation** with FastAPI, integrating all developed AI features (Talent Search, RatePricer, Matchmaker, Parsing)\\n\\n• **Dockerization, Cloud Run Deployment**, CI/CD with GitLab, and seamless integration into the Opteamis platform through close collaboration with backend and frontend teams\\n\\n• **Multi-tenant Architecture Design** to sell the solution to other clients\\n\\n• **MCP Server Development** that wraps our internal API, offering a ChatGPT-like interface for natural language interaction\\n\\n• **Google Chrome Extension Development** in Node.js for our Sales teams, which automatically centralizes downloaded CVs and interested candidates in a single email box\\n\\n• **Agent Sourcer Design**, a tool that automates sourcing by collecting and parsing data from multiple platforms (LinkedIn, Turnover, etc.)\\n\\n**Additional Projects:**\\n\\n• **Valoria Project**: Automation and collection of form response data for Valoria Label creation, for average score calculation for each company\'s respondents (HR/Direction/Collabs) in addition to BI reporting  and trophy display according to score obtained with AI fine-tuned on marketing team guidelines, enabling generation of recommendations and guidelines for each company\\n\\n• **Complete Configuration** of CI/CD, CloudSQL,OpenTelemetry and IAM Roles on GCP\\n\\n• **Meetings with AI SaaS Providers SaaS** to benchmark performances of other solutions in comparison with our solution.","technologies":["Agentic AI","GCP","Vertex AI","FastAPI","Dbt","Airbyte","Gitlab CI/CD","Vector Embeddings","FastMCP","Pinecone","Redis","Cloud Run","BigQuery","GCS","Docker","Qwen 3","Hugging Face Transformers","PCA","Catboost","Looker","OpenTelemetry"]},{"id":2,"position":"Freelance GenAI <span className=\\"font-serif\\">&</span> Data Analytics Engineer","company":"CHU Metz, Insurance, Finance","period":"September 2024 - Mars 2025","description":"• Supported multiple organizations (healthcare, accounting, insurance) in implementing cloud, data <span className=\\"font-serif\\">&</span> AI solutions\\n• Deployed Microsoft 365 Business environment <span className=\\"font-serif\\">&</span> Microsoft Fabric for CHU Metz\\n• Configured Azure Data Factory pipelines to enable Power BI Copilot <span className=\\"font-serif\\">&</span> industrialize data flows\\n• Implemented GCP IAM <span className=\\"font-serif\\">&</span> MFA governance <span className=\\"font-serif\\">&</span> deployed Qdrant vector database\\n• Designed AI-augmented BI pipelines connecting GPT via n8n to Power BI for semantic data exploration","detailedDescription":"As a Freelance GenAI <span className=\\"font-serif\\">&</span> Data Analytics Engineer, I supported multiple organizations (healthcare, accounting, insurance) in implementing and industrializing cloud, data <span className=\\"font-serif\\">&</span> AI solutions, from environment configuration to deployment of AI-augmented business features.\\n\\n**Key Realizations – CHU Metz:**\\n\\n• **Infrastructure <span className=\\"font-serif\\">&</span> Cloud**: Deployment of a Microsoft 365 Business environment and Microsoft Fabric, configuration of Azure Data Factory pipelines to enable Power BI Copilot and industrialize data flows.\\n\\n• **Security <span className=\\"font-serif\\">&</span> Data**: Implementation of cloud access governance IAM <span className=\\"font-serif\\">&</span> MFA on GCP and deployment of a Qdrant vector database to organize, store and query business data.\\n\\n• **AI <span className=\\"font-serif\\">&</span> Augmented BI**: Design of pipelines connecting GPT via n8n to Power BI, enabling AI-enriched report generation and semantic data exploration.","technologies":["Azure","Data Factory","Microsoft Fabric","Power BI Copilot","GCP","BigQuery","Qdrant","GPT","n8n","REST APIs","CI/CD","IAM","MFA"]},{"id":3,"position":"Data Analytics Engineer","company":"Soci\xe9t\xe9 G\xe9n\xe9rale - Banking","period":"March 2024 - September 2024","description":"• Designed <span className=\\"font-serif\\">&</span> automated dbt pipelines → Snowflake/Redshift for performance reports\\n• Modeled productivity ratios (OPE, OPP, FIP, FPE) to standardize practices\\n• Automated MicroStrategy <span className=\\"font-serif\\">&</span> Power BI dashboards for reliable KPI tracking\\n• Built credit monitoring datamarts for FPE business line\\n• Mentored interns on MicroStrategy BI <span className=\\"font-serif\\">&</span> data analysis best practices","detailedDescription":"As a Data Analytics Engineer at Soci\xe9t\xe9 G\xe9n\xe9rale, I focused on building robust data infrastructure and analytics solutions to support the bank\'s performance monitoring and reporting needs across multiple business lines.\\n\\n**Data Pipeline <span className=\\"font-serif\\">&</span> Infrastructure:**\\n\\n• **dbt Pipeline Design <span className=\\"font-serif\\">&</span> Automation**: Conception and automation of dbt pipelines → Snowflake/Redshift for performance report production covering 7 business lines and 21 service centers\\n\\n• **Data Warehouse Optimization**: Optimized Snowflake and AWS Redshift performance for large-scale data processing and analytics workloads\\n\\n• **ETL Process Enhancement**: Improved data extraction, transformation, and loading processes to ensure data quality and consistency\\n\\n• **Data Governance Implementation**: Established data governance frameworks and best practices for data lineage and quality control\\n\\n**Analytics <span className=\\"font-serif\\">&</span> Reporting:**\\n\\n• **Productivity Ratio Modeling**: Modeling of productivity ratios (OPE, OPP, FIP, FPE) to standardize practices and improve performance measurement\\n\\n• **Dashboard Automation**: Automation of MicroStrategy and Power BI dashboards, reducing manual updates and ensuring more reliable KPI tracking\\n\\n• **Credit Monitoring Datamarts**: Construction of credit monitoring datamarts in collaboration with the FPE business line (professionals and enterprises)\\n\\n• **Specific Indicators Implementation**: Implementation of specific indicators for successions (SUCC) and transfers/withdrawals (FTI) in Snowflake\\n\\n• **Real-time Reporting**: Development of real-time dashboards for critical business metrics and performance indicators\\n\\n• **Data Visualization Enhancement**: Created interactive visualizations using Power BI and MicroStrategy for better data insights\\n\\n**Business Support <span className=\\"font-serif\\">&</span> Collaboration:**\\n\\n• **Ad Hoc Support**: Management of ad hoc requests and support of business teams in their reporting needs\\n\\n• **Team Mentoring**: Mentoring of interns on MicroStrategy BI usage and data analysis best practices\\n\\n• **Strategic KPI Alignment**: Collaboration with Business Analysts and managers to define, document and align strategic KPIs in a common repository\\n\\n• **Stakeholder Communication**: Regular communication with business stakeholders to understand requirements and deliver solutions\\n\\n• **Documentation <span className=\\"font-serif\\">&</span> Training**: Created comprehensive documentation and conducted training sessions for data analysis tools and methodologies\\n\\n**Technical Implementation:**\\n\\n• **SQL Optimization**: Advanced SQL query optimization for improved performance and reduced processing time\\n\\n• **Python Scripting**: Developed Python scripts using Pandas for data manipulation and analysis automation\\n\\n• **CI/CD Integration**: Implemented GitLab CI/CD pipelines for automated testing and deployment of data models\\n\\n• **Cloud Infrastructure**: Managed AWS Lambda functions and CloudWatch monitoring for data processing workflows\\n\\n• **Data Quality Assurance**: Implemented automated data quality checks and validation processes","technologies":["Snowflake","AWS Redshift","dbt","SQL","Python","Pandas","MicroStrategy","Excel","Google Sheets","Power BI","GitLab CI/CD","AWS Lambda","CloudWatch"]},{"id":4,"position":"AI Engineer","company":"ETIS Research LAB - ENSEA","period":"June 2023 - September 2023","description":"• Developed <span className=\\"font-serif\\">&</span> optimized CNNs for computer vision (digit recognition, image classification, segmentation)\\n• Implemented LeNet-5 model in PyTorch/TensorFlow <span className=\\"font-serif\\">&</span> ported to C++/FPGA for optimized deployment\\n• Set up image preprocessing <span className=\\"font-serif\\">&</span> segmentation techniques to enhance model robustness\\n• Optimized FPGA inference via HLS with 97.23% accuracy <span className=\\"font-serif\\">&</span> 42.73 images/s throughput\\n• Benchmarked FPGA vs software with 3x latency reduction (0.0234s vs 0.0741s)","detailedDescription":"As an AI Engineer at ETIS Research LAB - ENSEA, I focused on developing and optimizing deep learning models for computer vision applications, with particular emphasis on FPGA acceleration for high-performance inference.\\n\\n**Key Realizations:**\\n\\n• **CNN Development <span className=\\"font-serif\\">&</span> Optimization**: Development and optimization of CNNs for computer vision (digit recognition, image classification, segmentation)\\n\\n• **LeNet-5 Implementation**: Implementation of the LeNet-5 model in PyTorch/TensorFlow and porting to C++/FPGA for optimized deployment\\n\\n• **Image Processing Pipeline**: Setup of image preprocessing and segmentation techniques to enhance model robustness\\n\\n• **FPGA Optimization**: Optimization of inference on FPGA via HLS, achieving 97.23% accuracy and 42.73 images/s throughput\\n\\n• **Performance Benchmarking**: FPGA vs software benchmarking, with 3x latency reduction (0.0234s vs 0.0741s)\\n\\n• **Model Architecture Design**: Custom CNN architectures for specific computer vision tasks\\n\\n• **Hardware-Software Co-design**: Integration of deep learning models with FPGA hardware for real-time processing\\n\\n• **Research <span className=\\"font-serif\\">&</span> Development**: Contribution to research projects in computer vision and embedded AI systems","technologies":["CNNs","LeNet-5","Computer Vision","Image Processing","PyTorch","TensorFlow","Keras","Python","C/C++","FPGA","HLS","VHDL","Verilog","Performance Benchmarking","Embedded Systems"]},{"id":5,"position":"Data Analytics Engineer","company":"Taza\'s Regional Government Office","period":"June 2022 - September 2022","description":"• Set up automated HR dashboards in Power BI <span className=\\"font-serif\\">&</span> Excel, eliminating manual updates\\n• Integrated Typeform responses via webhooks <span className=\\"font-serif\\">&</span> Cloud Run scripts for real-time processing\\n• Developed Python pipelines to clean data <span className=\\"font-serif\\">&</span> automatically calculate averages <span className=\\"font-serif\\">&</span> scores by team\\n• Automated HR KPI tracking (retention, turnover, absenteeism, file management)\\n• Designed demographic dashboards (age, seniority, contract type, department) with dynamic refresh","detailedDescription":"As a Data Analytics Engineer at Taza\'s Regional Government Office, I focused on automating HR processes and building comprehensive data analytics solutions to improve decision-making and operational efficiency across the organization.\\n\\n**Key Realizations:**\\n\\n• **Automated HR Dashboards**: Setup of automated HR dashboards in Power BI and Excel, eliminating manual updates\\n\\n• **Typeform Integration**: Integration of Typeform responses via webhooks and Cloud Run scripts for real-time processing\\n\\n• **Python Data Pipelines**: Development of Python pipelines to clean data and automatically calculate averages and scores by team\\n\\n• **HR KPI Automation**: Automation of HR KPI tracking (retention, turnover, absenteeism, file management)\\n\\n• **Demographic Dashboards**: Design of demographic dashboards (age, seniority, contract type, department) with dynamic refresh\\n\\n• **End-to-End Automation**: Deployment of end-to-end automation: new responses → scripts → SQL update → KPI recalculation → Power BI refresh\\n\\n• **Reporting Standardization**: Standardization of reporting between HR units, improving decision-making and reducing ad hoc tasks\\n\\n• **Data Integration**: API Typeform integration and automated payload processing for seamless data flow","technologies":["SQL","Python","Pandas","NumPy","VBA","API","Webhooks","Cloud Run","Power BI","Excel","Google Sheets","Typeform API","Data Integration"]}]')},6064:function(e){e.exports=JSON.parse('[{"id":1,"title":"Healthcare Data Platform","client":"CHU Metz","sector":"Healthcare","period":"2023","description":"Deployed a comprehensive cloud & AI infrastructure for secure data governance, built vector databases for medical records, and enabled AI-powered reporting systems.","technologies":["AWS","Python","FastAPI","PostgreSQL","Vector DBs","AI APIs"],"testimonial":{"text":"Nasrallah delivered an exceptional AI infrastructure that transformed our data management. His expertise in cloud technologies and attention to security requirements exceeded our expectations.","author":"Dr. Marie Dubois","position":"IT Director, CHU Metz","rating":5},"screenshot":"/screenshots/chu-metz-dashboard.jpg"},{"id":2,"title":"Insurance Analytics Pipeline","client":"Confidential Insurance","sector":"Insurance","period":"2022-2023","description":"Built automated risk assessment models and industrialized analytics pipelines, reducing manual processing time by 70% and improving accuracy by 40%.","technologies":["Python","Apache Airflow","Machine Learning","Snowflake","dbt"],"testimonial":{"text":"The analytics pipeline Nasrallah built revolutionized our risk assessment process. The accuracy improvements and time savings have been game-changing for our operations.","author":"Jean-Pierre Martin","position":"Head of Data Analytics","rating":5},"screenshot":"/screenshots/insurance-analytics.jpg"},{"id":3,"title":"Financial AI Solutions","client":"Financial Services Firm","sector":"Finance","period":"2022","description":"Developed custom AI models for fraud detection and automated reporting systems, processing millions of transactions with 99.5% accuracy.","technologies":["Python","TensorFlow","Kubernetes","Redis","PostgreSQL"],"testimonial":{"text":"Nasrallah\'s AI solutions have significantly improved our fraud detection capabilities. His technical expertise and project management skills are outstanding.","author":"Sophie Laurent","position":"CTO","rating":5},"screenshot":"/screenshots/finance-ai-dashboard.jpg"},{"id":4,"title":"Research AI Optimization","client":"ENSEA Research Lab","sector":"Research","period":"2021-2022","description":"Designed and optimized deep learning models (CNNs) with FPGA acceleration for computer vision tasks, achieving 3x performance improvement.","technologies":["Python","PyTorch","FPGA","Computer Vision","Deep Learning"],"testimonial":{"text":"Nasrallah\'s work on FPGA acceleration was groundbreaking. His optimization techniques pushed the boundaries of what we thought possible with our hardware.","author":"Prof. Alexandre Moreau","position":"Research Director, ENSEA","rating":5},"screenshot":"/screenshots/research-fpga.jpg"}]')}}]);